{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development\n",
    "\n",
    "This section details the model selection process and initial training procedures.\n",
    "Navigate: [Previous (Data Preparation)](01_data_preparation.ipynb) | [Next (Feature Engineering)](03_feature_engineering.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Splitting to training and testing data\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Connect and get comprehensive data\n",
    "conn = sqlite3.connect('mental_health_final.db')\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    e.employee_id,\n",
    "    e.age,\n",
    "    e.gender,\n",
    "    e.country,\n",
    "    emp.company_size,\n",
    "    emp.is_tech_company,\n",
    "    emp.work_remotely,\n",
    "    mhb.has_mental_health_benefits,\n",
    "    mhh.current_disorder,\n",
    "    mhh.sought_treatment,\n",
    "    wc.discuss_with_supervisor\n",
    "FROM employees e\n",
    "LEFT JOIN employment emp ON e.employee_id = emp.employee_id\n",
    "LEFT JOIN mental_health_benefits mhb ON e.employee_id = mhb.employee_id\n",
    "LEFT JOIN mental_health_history mhh ON e.employee_id = mhh.employee_id\n",
    "LEFT JOIN workplace_communication wc ON e.employee_id = wc.employee_id\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Explore potential target variables distribution\n",
    "print(\"\\nDistribution of key variables:\")\n",
    "print(\"\\nCurrent Mental Health Disorder:\")\n",
    "print(df['current_disorder'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nSought Treatment:\")\n",
    "print(df['sought_treatment'].value_counts(normalize=True))\n",
    "\n",
    "print(\"\\nTech Company Distribution:\")\n",
    "print(df['is_tech_company'].value_counts(normalize=True))\n",
    "\n",
    "# Check gender distribution\n",
    "print(\"\\nGender Distribution:\")\n",
    "print(df['gender'].value_counts(normalize=True))\n",
    "\n",
    "# Check country distribution\n",
    "print(\"\\nCountry Distribution:\")\n",
    "print(df['country'].value_counts(normalize=True))\n",
    "\n",
    "# Visualize distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Current Disorder Distribution\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.countplot(data=df, x='current_disorder')\n",
    "plt.title('Distribution of Current Mental Health Disorder')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot 2: Sought Treatment Distribution\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.countplot(data=df, x='sought_treatment')\n",
    "plt.title('Distribution of Treatment Seeking')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot 3: Tech Company Distribution\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.countplot(data=df, x='is_tech_company')\n",
    "plt.title('Distribution of Tech vs Non-Tech Companies')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Plot 4: Gender Distribution\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.countplot(data=df, y='gender')\n",
    "plt.title('Gender Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Based on exploration, we'll use 'sought_treatment' as target variable\n",
    "# and stratify by this and gender to maintain distributions\n",
    "\n",
    "# Prepare features and target\n",
    "X = df.drop(['employee_id', 'sought_treatment'], axis=1)\n",
    "y = df['sought_treatment']\n",
    "\n",
    "# Handle categorical variables\n",
    "categorical_cols = ['gender', 'country', 'company_size', 'work_remotely', \n",
    "                   'has_mental_health_benefits', 'current_disorder', \n",
    "                   'discuss_with_supervisor']\n",
    "\n",
    "# Create dummy variables\n",
    "X = pd.get_dummies(X, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Perform train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Verify stratification\n",
    "print(\"\\nTraining set distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nTest set distribution:\")\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "# Save the splits\n",
    "print(\"\\nDataset shapes:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Yprofile and correlation matrix\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Get data from database\n",
    "conn = sqlite3.connect('mental_health_final.db')\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    e.employee_id,\n",
    "    e.age,\n",
    "    e.gender,\n",
    "    e.country,\n",
    "    emp.company_size,\n",
    "    emp.is_tech_company,\n",
    "    emp.work_remotely,\n",
    "    mhb.has_mental_health_benefits,\n",
    "    mhb.knows_benefits_options,\n",
    "    mhb.mental_health_resources,\n",
    "    mhb.leave_request_ease,\n",
    "    mhh.family_history,\n",
    "    mhh.current_disorder,\n",
    "    mhh.sought_treatment,\n",
    "    wc.discuss_with_supervisor,\n",
    "    wc.discuss_with_coworkers,\n",
    "    wc.observed_negative_consequences,\n",
    "    wc.interferes_with_work\n",
    "FROM employees e\n",
    "LEFT JOIN employment emp ON e.employee_id = emp.employee_id\n",
    "LEFT JOIN mental_health_benefits mhb ON e.employee_id = mhb.employee_id\n",
    "LEFT JOIN mental_health_history mhh ON e.employee_id = mhh.employee_id\n",
    "LEFT JOIN workplace_communication wc ON e.employee_id = wc.employee_id\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql_query(query, conn)\n",
    "conn.close()\n",
    "\n",
    "# Generate YData Profile Report\n",
    "profile = ProfileReport(df, title=\"Mental Health Survey Data Profiling Report\")\n",
    "profile.to_file(\"mental_health_profile_report.html\")\n",
    "\n",
    "# Create correlation matrix\n",
    "# First, convert categorical variables to numeric\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    df[col] = pd.factorize(df[col])[0]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True,\n",
    "            cmap='coolwarm',\n",
    "            center=0,\n",
    "            fmt='.2f',\n",
    "            square=True)\n",
    "plt.title('Correlation Matrix of Mental Health Survey Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Print strongest correlations\n",
    "print(\"\\nStrongest Feature Correlations:\")\n",
    "corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if i != j:\n",
    "            corr_pairs.append((\n",
    "                correlation_matrix.index[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "# Sort by absolute correlation value\n",
    "corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "\n",
    "# Print top 15 correlations\n",
    "print(\"\\nTop 15 Feature Correlations:\")\n",
    "for pair in corr_pairs[:15]:\n",
    "    print(f\"{pair[0]} - {pair[1]}: {pair[2]:.3f}\")\n",
    "\n",
    "# Additional summary statistics for numerical variables\n",
    "print(\"\\nNumerical Variables Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Distribution of key categorical variables\n",
    "print(\"\\nCategorical Variables Distribution:\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col} distribution:\")\n",
    "    print(df[col].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Dagshub and MLflow connection\n",
    "import dagshub\n",
    "dagshub.init(repo_owner='ashiashish100', repo_name='my-first-repo', mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Experiment 1\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import logging\n",
    "\n",
    "# Configure MLflow to track experiments on DagsHub\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/ashiashish100/my-first-repo.mlflow\")\n",
    "mlflow.set_experiment(\"mental_health_prediction\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def standardize_gender(df):\n",
    "    #Standardize gender categories\n",
    "    df = df.copy()\n",
    "    # Convert to lowercase\n",
    "    df['gender'] = df['gender'].str.lower()\n",
    "    \n",
    "    # Create mapping for gender standardization\n",
    "    gender_map = {\n",
    "        'male': 'male',\n",
    "        'm': 'male',\n",
    "        'man': 'male',\n",
    "        'cis male': 'male',\n",
    "        'male ': 'male',\n",
    "        'cisdude': 'male',\n",
    "        'm|': 'male',\n",
    "        'female': 'female',\n",
    "        'f': 'female',\n",
    "        'woman': 'female',\n",
    "        'cis female': 'female',\n",
    "        'female ': 'female'\n",
    "    }\n",
    "    \n",
    "    # Apply mapping and group all other values as 'other'\n",
    "    df['gender'] = df['gender'].apply(lambda x: gender_map.get(str(x).lower(), 'other'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    #Load and preprocess the data\n",
    "    logger.info(\"Loading data from database...\")\n",
    "    conn = sqlite3.connect('mental_health_final.db')\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        e.age,\n",
    "        e.gender,\n",
    "        e.country,\n",
    "        emp.company_size,\n",
    "        emp.is_tech_company,\n",
    "        emp.work_remotely,\n",
    "        mhb.has_mental_health_benefits,\n",
    "        mhh.current_disorder,\n",
    "        mhh.sought_treatment\n",
    "    FROM employees e\n",
    "    LEFT JOIN employment emp ON e.employee_id = emp.employee_id\n",
    "    LEFT JOIN mental_health_benefits mhb ON e.employee_id = mhb.employee_id\n",
    "    LEFT JOIN mental_health_history mhh ON e.employee_id = mhh.employee_id\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    logger.info(\"Preprocessing data...\")\n",
    "    # Standardize gender categories\n",
    "    df = standardize_gender(df)\n",
    "    \n",
    "    # Handle NaN values\n",
    "    df['age'] = df['age'].fillna(df['age'].median())\n",
    "    df['is_tech_company'] = df['is_tech_company'].fillna(0)\n",
    "    \n",
    "    categorical_columns = ['gender', 'country', 'company_size', 'work_remotely', \n",
    "                         'has_mental_health_benefits', 'current_disorder']\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        df[col] = df[col].fillna('unknown')\n",
    "    \n",
    "    logger.info(f\"Data loaded and preprocessed. Shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def create_pipeline(scaler_type='standard'):\n",
    "    #Create preprocessing and model pipeline\n",
    "    logger.info(f\"Creating pipeline with {scaler_type} scaler...\")\n",
    "    \n",
    "    numeric_features = ['age', 'is_tech_company']\n",
    "    categorical_features = ['gender', 'country', 'company_size', 'work_remotely', \n",
    "                          'has_mental_health_benefits', 'current_disorder']\n",
    "    \n",
    "    # Create preprocessing pipelines\n",
    "    if scaler_type == 'standard':\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "    elif scaler_type == 'minmax':\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('scaler', MinMaxScaler())\n",
    "        ])\n",
    "    elif scaler_type == 'log':\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('log', FunctionTransformer(np.log1p)),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "    ])\n",
    "    \n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Create full pipeline\n",
    "    return Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(random_state=42))\n",
    "    ])\n",
    "\n",
    "def run_experiment(X_train, X_test, y_train, y_test, scaler_type):\n",
    "    #Run experiment and log results\n",
    "    logger.info(f\"Starting experiment with {scaler_type} scaler...\")\n",
    "    \n",
    "    with mlflow.start_run(run_name=f\"mental_health_{scaler_type}_scaler\"):\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"scaler_type\", scaler_type)\n",
    "        mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "        mlflow.log_param(\"random_state\", 42)\n",
    "        \n",
    "        # Create and train pipeline\n",
    "        pipeline = create_pipeline(scaler_type)\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"f1_score\", f1)\n",
    "        mlflow.log_metric(\"true_positives\", tp)\n",
    "        mlflow.log_metric(\"true_negatives\", tn)\n",
    "        mlflow.log_metric(\"false_positives\", fp)\n",
    "        mlflow.log_metric(\"false_negatives\", fn)\n",
    "        \n",
    "        # Calculate and log additional metrics\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        \n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision)\n",
    "        mlflow.log_metric(\"recall\", recall)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(pipeline, \"model\")\n",
    "        \n",
    "        logger.info(f\"Experiment with {scaler_type} scaler completed:\")\n",
    "        logger.info(f\"F1 Score: {f1:.4f}\")\n",
    "        logger.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "        logger.info(f\"Precision: {precision:.4f}\")\n",
    "        logger.info(f\"Recall: {recall:.4f}\")\n",
    "        logger.info(f\"Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
    "        \n",
    "        return pipeline, f1\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logger.info(\"Starting mental health prediction experiment...\")\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        df = load_and_preprocess_data()\n",
    "        \n",
    "        # Prepare features and target\n",
    "        X = df.drop('sought_treatment', axis=1)\n",
    "        y = df['sought_treatment']\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Train set shape: {X_train.shape}\")\n",
    "        logger.info(f\"Test set shape: {X_test.shape}\")\n",
    "        \n",
    "        # Run experiments with different scalers\n",
    "        scalers = ['standard', 'minmax', 'log']\n",
    "        results = {}\n",
    "        \n",
    "        for scaler in scalers:\n",
    "            logger.info(f\"\\nRunning experiment with {scaler} scaler...\")\n",
    "            pipeline, f1 = run_experiment(X_train, X_test, y_train, y_test, scaler)\n",
    "            results[scaler] = {'pipeline': pipeline, 'f1_score': f1}\n",
    "        \n",
    "        # Print final results\n",
    "        logger.info(\"\\nFinal Results:\")\n",
    "        for scaler, result in results.items():\n",
    "            logger.info(f\"{scaler} scaler - F1 Score: {result['f1_score']:.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {str(e)}\")\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Experiment 2\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from xgboost import XGBClassifier\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import logging\n",
    "\n",
    "# Set DagsHub credentials\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'ashiashish100'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = '7af28a60cc2f6e231f6413c9b48e241766a2e931'\n",
    "\n",
    "# MLflow setup\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/ashiashish100/my-first-repo.mlflow\")\n",
    "mlflow.set_experiment(\"mental_health_multi_classifier\")\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def standardize_gender(df):\n",
    "    \"\"\"Standardize gender categories\"\"\"\n",
    "    df = df.copy()\n",
    "    # Convert to lowercase\n",
    "    df['gender'] = df['gender'].str.lower()\n",
    "    \n",
    "    # Create mapping for gender standardization\n",
    "    gender_map = {\n",
    "        'male': 'male',\n",
    "        'm': 'male',\n",
    "        'man': 'male',\n",
    "        'cis male': 'male',\n",
    "        'male ': 'male',\n",
    "        'cisdude': 'male',\n",
    "        'm|': 'male',\n",
    "        'female': 'female',\n",
    "        'f': 'female',\n",
    "        'woman': 'female',\n",
    "        'cis female': 'female',\n",
    "        'female ': 'female'\n",
    "    }\n",
    "    \n",
    "    # Apply mapping and group all other values as 'other'\n",
    "    df['gender'] = df['gender'].apply(lambda x: gender_map.get(str(x).lower(), 'other'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    logger.info(\"Loading data from database...\")\n",
    "    conn = sqlite3.connect('mental_health_final.db')\n",
    "    query = \"\"\"\n",
    "    SELECT \n",
    "        e.age,\n",
    "        e.gender,\n",
    "        e.country,\n",
    "        emp.company_size,\n",
    "        emp.is_tech_company,\n",
    "        emp.work_remotely,\n",
    "        mhb.has_mental_health_benefits,\n",
    "        mhh.current_disorder,\n",
    "        mhh.sought_treatment\n",
    "    FROM employees e\n",
    "    LEFT JOIN employment emp ON e.employee_id = emp.employee_id\n",
    "    LEFT JOIN mental_health_benefits mhb ON e.employee_id = mhb.employee_id\n",
    "    LEFT JOIN mental_health_history mhh ON e.employee_id = mhh.employee_id\n",
    "    \"\"\"\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    logger.info(\"Preprocessing data...\")\n",
    "    # Standardize gender categories\n",
    "    df = standardize_gender(df)\n",
    "    \n",
    "    # Handle NaN values\n",
    "    df['age'] = df['age'].fillna(df['age'].median())\n",
    "    df['is_tech_company'] = df['is_tech_company'].fillna(0)\n",
    "    \n",
    "    categorical_columns = ['gender', 'country', 'company_size', 'work_remotely', \n",
    "                         'has_mental_health_benefits', 'current_disorder']\n",
    "    \n",
    "    for col in categorical_columns:\n",
    "        df[col] = df[col].fillna('unknown')\n",
    "    \n",
    "    logger.info(f\"Data loaded and preprocessed. Shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "def create_preprocessing_pipeline():\n",
    "    #Create preprocessing pipeline\n",
    "    numeric_features = ['age', 'is_tech_company']\n",
    "    categorical_features = ['gender', 'country', 'company_size', 'work_remotely', \n",
    "                          'has_mental_health_benefits', 'current_disorder']\n",
    "    \n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "def evaluate_classifier(clf, X_train, X_test, y_train, y_test):\n",
    "    #Evaluate classifier and return metrics\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_test, y_pred),\n",
    "        'f1_score': f1_score(y_test, y_pred),\n",
    "        'precision': precision_score(y_test, y_pred),\n",
    "        'recall': recall_score(y_test, y_pred)\n",
    "    }\n",
    "    \n",
    "    return metrics, clf\n",
    "\n",
    "def run_experiment(classifier, clf_name, preprocessor, X_train, X_test, y_train, y_test):\n",
    "    #Run experiment for a classifier and log results to MLflow\n",
    "    with mlflow.start_run(run_name=f\"{clf_name}_experiment\"):\n",
    "        # Create and train pipeline\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', classifier)\n",
    "        ])\n",
    "        \n",
    "        # Log classifier parameters\n",
    "        params = classifier.get_params()\n",
    "        for param_name, param_value in params.items():\n",
    "            mlflow.log_param(param_name, param_value)\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='f1')\n",
    "        \n",
    "        # Train and evaluate on test set\n",
    "        metrics, trained_pipeline = evaluate_classifier(pipeline, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"cv_f1_mean\", cv_scores.mean())\n",
    "        mlflow.log_metric(\"cv_f1_std\", cv_scores.std())\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            mlflow.log_metric(f\"test_{metric_name}\", metric_value)\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(trained_pipeline, \"model\")\n",
    "        \n",
    "        # Print results\n",
    "        logger.info(f\"\\nResults for {clf_name}:\")\n",
    "        logger.info(f\"CV F1-Score: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "        logger.info(f\"Test Metrics:\")\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            logger.info(f\"{metric_name}: {metric_value:.4f}\")\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data using the new function\n",
    "    df = load_and_preprocess_data()\n",
    "    \n",
    "    # Prepare features and target\n",
    "    X = df.drop('sought_treatment', axis=1)\n",
    "    y = df['sought_treatment']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Create preprocessor\n",
    "    preprocessor = create_preprocessing_pipeline()\n",
    "    \n",
    "    # Define classifiers\n",
    "    classifiers = {\n",
    "        'LogisticRegression': LogisticRegression(random_state=42),\n",
    "        'RidgeClassifier': RidgeClassifier(random_state=42),\n",
    "        'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'XGBoost': XGBClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Run experiments for each classifier\n",
    "    for clf_name, classifier in classifiers.items():\n",
    "        logger.info(f\"\\nRunning experiment with {clf_name}\")\n",
    "        run_experiment(\n",
    "            classifier, clf_name, preprocessor, X_train, X_test, y_train, y_test\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
